{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6557693,"sourceType":"datasetVersion","datasetId":3789174},{"sourceId":6618183,"sourceType":"datasetVersion","datasetId":3819589}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# **Conceptual Graph-Based Recommendation System for PRM Issues: OL Approach**\nWelcome to this notebook dedicated to the exploration and implementation of a cutting-edge recommendation system designed to address Public Relations Management (PRM) issues. This innovative system leverages the power of Conceptual Graphs and employs an Online Learning (OL) approach to provide tailored solutions for PRM challenges.\n\nIn this notebook, we will delve into the foundations of conceptual graphs, their role in recommendation systems, and how the online learning paradigm enhances their effectiveness. We will walk through the step-by-step process of building and deploying this recommendation system, aiming to empower PRM professionals with a powerful tool to make informed decisions and optimize their strategies.\n\nBy the end of this notebook, you will have a comprehensive understanding of how to implement and customize this recommendation system to suit your specific PRM needs. Let's embark on this exciting journey towards revolutionizing your PRM strategies!","metadata":{}},{"cell_type":"markdown","source":"## Dependencies Installation","metadata":{}},{"cell_type":"code","source":"!pip install pdfplumber\n!pip install autocorrect\n!pip install stanza\n!pip install PyMuPDF\n!pip install transformers==4.12.0\n!pip install PyPDF2\n!pip install transformers\n!pip install textacy\n!pip install rouge\n!pip install sentence-transformers\n!pip install faiss-cpu --no-cache","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:15:45.845801Z","iopub.execute_input":"2024-05-04T12:15:45.846093Z","iopub.status.idle":"2024-05-04T12:18:18.804533Z","shell.execute_reply.started":"2024-05-04T12:15:45.846068Z","shell.execute_reply":"2024-05-04T12:18:18.803424Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pdfplumber\n  Downloading pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from pdfplumber) (9.5.0)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.29.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (3.1.0)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber) (38.0.4)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\nInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\nSuccessfully installed pdfminer.six-20231228 pdfplumber-0.11.0 pypdfium2-4.29.0\nCollecting autocorrect\n  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=cca2807ff2daab6f93b65f5a0206f202ff1c71be1b4085bf4d88b107667c661f\n  Stored in directory: /root/.cache/pip/wheels/b5/7b/6d/b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\nCollecting stanza\n  Downloading stanza-1.8.2-py3-none-any.whl (990 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.8.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.23.5)\nRequirement already satisfied: protobuf>=3.15.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.31.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from stanza) (3.1)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from stanza) (0.10.2)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\nInstalling collected packages: stanza\nSuccessfully installed stanza-1.8.2\nCollecting PyMuPDF\n  Downloading PyMuPDF-1.24.2-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting PyMuPDFb==1.24.1 (from PyMuPDF)\n  Downloading PyMuPDFb-1.24.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\nSuccessfully installed PyMuPDF-1.24.2 PyMuPDFb-1.24.1\nCollecting transformers==4.12.0\n  Downloading transformers-4.12.0-py3-none-any.whl (3.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.12.0) (3.12.2)\nRequirement already satisfied: huggingface-hub>=0.0.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.12.0) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.12.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.12.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.12.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.12.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.12.0) (2.31.0)\nCollecting sacremoses (from transformers==4.12.0)\n  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.0)\n  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l/^C\n\u001b[?25canceled\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mCollecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nCollecting textacy\n  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (4.2.4)\nRequirement already satisfied: catalogue~=2.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (2.0.9)\nRequirement already satisfied: cytoolz>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from textacy) (0.12.2)\nCollecting floret~=0.10.0 (from textacy)\n  Downloading floret-0.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting jellyfish>=0.8.0 (from textacy)\n  Downloading jellyfish-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.3.2)\nRequirement already satisfied: networkx>=2.7 in /opt/conda/lib/python3.10/site-packages (from textacy) (3.1)\nRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.23.5)\nCollecting pyphen>=0.10.0 (from textacy)\n  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (2.31.0)\nRequirement already satisfied: scipy>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.11.2)\nRequirement already satisfied: scikit-learn>=1.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (1.2.2)\nRequirement already satisfied: spacy~=3.0 in /opt/conda/lib/python3.10/site-packages (from textacy) (3.6.1)\nRequirement already satisfied: tqdm>=4.19.6 in /opt/conda/lib/python3.10/site-packages (from textacy) (4.66.1)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.10.0->textacy) (2023.7.22)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.0->textacy) (3.1.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.0.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.0.9)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (2.0.7)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.0.8)\nRequirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (8.1.12)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (2.4.7)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (0.9.0)\nRequirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (0.10.1)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (6.3.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (1.10.9)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (68.0.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy~=3.0->textacy) (3.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy~=3.0->textacy) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.6.3)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.1.1)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy~=3.0->textacy) (2.1.3)\nInstalling collected packages: pyphen, jellyfish, floret, textacy\nSuccessfully installed floret-0.10.5 jellyfish-1.0.3 pyphen-0.15.0 textacy-0.13.0\nCollecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\nCollecting sentence-transformers\n  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n  Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.2)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.16.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nCollecting huggingface-hub>=0.15.1 (from sentence-transformers)\n  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.6.3)\nCollecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.3.3\n    Uninstalling safetensors-0.3.3:\n      Successfully uninstalled safetensors-0.3.3\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.16.4\n    Uninstalling huggingface-hub-0.16.4:\n      Successfully uninstalled huggingface-hub-0.16.4\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.13.3\n    Uninstalling tokenizers-0.13.3:\n      Successfully uninstalled tokenizers-0.13.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.33.0\n    Uninstalling transformers-4.33.0:\n      Successfully uninstalled transformers-4.33.0\nSuccessfully installed huggingface-hub-0.23.0 safetensors-0.4.3 sentence-transformers-2.7.0 tokenizers-0.19.1 transformers-4.40.1\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.23.5)\nInstalling collected packages: faiss-cpu\nSuccessfully installed faiss-cpu-1.8.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from rouge import Rouge\nimport  pdfplumber\nimport string\nimport re\nimport stanza\nfrom transformers import pipeline\nimport pandas as pd\n#Download the Stanza model for your desired language (e.g., English)\nstanza.download('en')\nnlp = stanza.Pipeline(lang='en', processors='tokenize,pos')\n#fix_spelling = pipeline(\"text2text-generation\",model=\"oliverguhr/spelling-correction-english-base\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:18:18.806306Z","iopub.execute_input":"2024-05-04T12:18:18.806620Z","iopub.status.idle":"2024-05-04T12:19:18.400524Z","shell.execute_reply.started":"2024-05-04T12:18:18.806591Z","shell.execute_reply":"2024-05-04T12:19:18.399741Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f86b13f651914cb8868ccba3e5671865"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip:   0%|          | 0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0640d2e3416f4e18a3873deab12899f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c288425dfacf475db5177dbe74b6b28a"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Extract Concepts","metadata":{}},{"cell_type":"markdown","source":"* After obtaining the raw text data, the subsequent step involved data engineering. This process focused on distilling essential information from the raw text to form a well-organized and lucid dataframe, which would serve as the foundation for constructing the knowledge graph and ontology.\n\n* To initiate this phase, we employed the Stanza matcher to identify and extract key concepts. By specifying patterns corresponding to these concepts, we effectively configured the matcher. Additionally, we meticulously reviewed and rectified any incorrect matches to enhance the accuracy of the Stanza matcher for our specific case. This diligence paid off, resulting in the successful extraction of all relevant concepts.","metadata":{}},{"cell_type":"code","source":"import fitz  # PyMuPDF\nimport re\nimport torch\nfrom transformers import BertTokenizer, BertForMaskedLM\n\n# Charger le modèle BERT\nmodel_name = \"bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForMaskedLM.from_pretrained(model_name)\n\ndef extract_titles_and_descriptions_from_pdf(pdf_file):\n    pdf_document = fitz.open(pdf_file)\n    titles_with_descriptions = []\n\n    for page_num in range(len(pdf_document)):\n        page = pdf_document[page_num]\n        page_text = page.get_text()\n        page_text=page_text.replace('—','\\n')\n        # Utilisez une expression régulière pour rechercher des motifs de titre \"11.x\" ou \"11.x.x\" ou \"11.x.x.x\"\n        matches = re.findall(r'('+'11+(\\.\\d+)+)\\s+(.+)', page_text)\n        for match in matches:\n            title = match[0]\n            description = match[2]\n\n            # Prédire la partie manquante de la description en utilisant BERT\n            masked_text = f\"[MASK] {description}\"\n            input_ids = tokenizer.encode(masked_text, add_special_tokens=True, return_tensors=\"pt\")\n            mask_index = input_ids[0].tolist().index(tokenizer.mask_token_id)\n\n            with torch.no_grad():\n                predictions = model(input_ids)\n            predicted_token_id = torch.argmax(predictions.logits[0, mask_index]).item()\n            predicted_word = tokenizer.decode(predicted_token_id)\n\n            # Remplacez le masque par le mot prédit dans la description\n            description = description.replace(\"[MASK]\", predicted_word)\n\n            # Ajoutez le titre et la description complète à la liste\n            titles_with_descriptions.append(f\"{title} {description}\")\n\n    pdf_document.close()\n    return titles_with_descriptions\n\n# Exemple d'utilisation\npdf_file_path = '/kaggle/input/prm-pmbok6-2017/PRM-PMBOK6-2017 (1).pdf'\ntitles_with_descriptions = extract_titles_and_descriptions_from_pdf(pdf_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:00:05.615695Z","iopub.execute_input":"2023-10-10T10:00:05.616184Z","iopub.status.idle":"2023-10-10T10:00:25.723797Z","shell.execute_reply.started":"2023-10-10T10:00:05.616139Z","shell.execute_reply":"2023-10-10T10:00:25.722589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef extract_Figure_from_pdf(pdf_file):\n    pdf_document = fitz.open(pdf_file)\n    titles_with_descriptions = []\n\n    for page_num in range(len(pdf_document)):\n        page = pdf_document[page_num]\n        page_text = page.get_text()\n        page_text=page_text.replace('—','\\n')\n        #print(page_text)\n        # Utilisez une expression régulière pour rechercher des motifs de titre \"11.x\" ou \"11.x.x\" ou \"11.x.x.x\"\n        matches = re.findall(r'^(Figure)+\\s([1-9][0-9]?|100)+-+([1-9][0-9]?|100)+.', page_text, re.MULTILINE)\n        for match in matches:\n            title = match[0]\n            title2 = match[1]\n            description = match[2]\n\n            # Prédire la partie manquante de la description en utilisant BERT\n            masked_text = f\"[MASK] {description}\"\n            input_ids = tokenizer.encode(masked_text, add_special_tokens=True, return_tensors=\"pt\")\n            mask_index = input_ids[0].tolist().index(tokenizer.mask_token_id)\n\n            with torch.no_grad():\n                predictions = model(input_ids)\n            predicted_token_id = torch.argmax(predictions.logits[0, mask_index]).item()\n            predicted_word = tokenizer.decode(predicted_token_id)\n\n            # Remplacez le masque par le mot prédit dans la description\n            description = description.replace(\"[MASK]\", predicted_word)\n\n            # Ajoutez le titre et la description complète à la liste\n            titles_with_descriptions.append(f\"{title} {title2}-{description}\")\n\n    pdf_document.close()\n    return titles_with_descriptions\n\n# Exemple d'utilisation\npdf_file_path = '/kaggle/input/prm-pmbok6-2017/PRM-PMBOK6-2017 (1).pdf'\nFigure = extract_Figure_from_pdf(pdf_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T13:54:45.723289Z","iopub.execute_input":"2023-10-09T13:54:45.724260Z","iopub.status.idle":"2023-10-09T13:54:47.643786Z","shell.execute_reply.started":"2023-10-09T13:54:45.724220Z","shell.execute_reply":"2023-10-09T13:54:47.642831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract using pdfplumber into text corpus","metadata":{}},{"cell_type":"code","source":"file = open('/kaggle/input/prm-pmbok6-2017/PRM-PMBOK6-2017 (1).pdf','rb')\nproject_risk_management = ''\nwith pdfplumber.open(file) as pdf:\n    for i in range(0,64):\n        page = pdf.pages[i].filter(lambda obj: not (obj[\"object_type\"] == \"char\" and obj[\"size\"] > 30))\n        project_risk_management += page.extract_text()\nproject_risk_management = project_risk_management.replace('\\n','\\n ')\nproject_risk_management = project_risk_management.replace('  ',' ')\nproject_risk_management = project_risk_management.lower()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:00:38.778113Z","iopub.execute_input":"2023-10-10T10:00:38.778529Z","iopub.status.idle":"2023-10-10T10:00:47.668143Z","shell.execute_reply.started":"2023-10-10T10:00:38.778496Z","shell.execute_reply":"2023-10-10T10:00:47.667078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Auto correct","metadata":{}},{"cell_type":"markdown","source":"using spelling-correction-english-base model we correct the corpus ","metadata":{}},{"cell_type":"code","source":"#fix_spelling = pipeline(\"text2text-generation\",model=\"oliverguhr/spelling-correction-english-base\")","metadata":{"execution":{"iopub.status.busy":"2023-10-07T15:41:04.371518Z","iopub.execute_input":"2023-10-07T15:41:04.372100Z","iopub.status.idle":"2023-10-07T15:41:04.376401Z","shell.execute_reply.started":"2023-10-07T15:41:04.372067Z","shell.execute_reply":"2023-10-07T15:41:04.375492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"#Example text (you can replace this with your corpus)\n#Process the corpus text\ndoc = nlp(project_risk_management)\nl=[]\n#Iterate through each sentence\nfor sentence in doc.sentences:\n    # Access the text of the sentence\n    sentence_text = \" \".join([word.text for word in sentence.words])\n\n#Print the sentence text\n    l.append(fix_spelling(sentence_text,max_length=2048)[0]['generated_text'])\nproject_risk_management=\" \".join(l)    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-10-07T15:41:04.377752Z","iopub.execute_input":"2023-10-07T15:41:04.378323Z","iopub.status.idle":"2023-10-07T15:41:04.392088Z","shell.execute_reply.started":"2023-10-07T15:41:04.378294Z","shell.execute_reply":"2023-10-07T15:41:04.390897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract definition using gpt2","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# Load the pretrained GPT-2 model and tokenizer\nmodel_name = \"gpt2-medium\"  # Choose a GPT-2 variant\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:00:47.669710Z","iopub.execute_input":"2023-10-10T10:00:47.670080Z","iopub.status.idle":"2023-10-10T10:01:27.071185Z","shell.execute_reply.started":"2023-10-10T10:00:47.670043Z","shell.execute_reply":"2023-10-10T10:01:27.069986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = open('output.txt','w')\nfile.write(project_risk_management)\nfile.close()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:27.073558Z","iopub.execute_input":"2023-10-10T10:01:27.074638Z","iopub.status.idle":"2023-10-10T10:01:27.081400Z","shell.execute_reply.started":"2023-10-10T10:01:27.074597Z","shell.execute_reply":"2023-10-10T10:01:27.080433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load and preprocess your fine-tuning dataset\n# Replace 'your_dataset.txt' with the path to your dataset\ndataset = TextDataset(\n    tokenizer=tokenizer,\n    file_path='/kaggle/working/output.txt',\n    block_size=128,  # Adjust block size as needed\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=False,\n)\n\n# Configure training arguments\ntraining_args = TrainingArguments(\n    output_dir='./fine-tuned-gpt2',  # Specify the output directory\n    overwrite_output_dir=True,\n    num_train_epochs=20,  # Adjust the number of training epochs\n    per_device_train_batch_size=16,  # Adjust batch size as needed\n    save_steps=10_000,  # Specify how often to save the model\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model()\n\n# You can now use the fine-tuned model for text generation tasks","metadata":{"execution":{"iopub.status.busy":"2023-10-09T13:56:11.933620Z","iopub.execute_input":"2023-10-09T13:56:11.933947Z","iopub.status.idle":"2023-10-09T13:59:57.485440Z","shell.execute_reply.started":"2023-10-09T13:56:11.933924Z","shell.execute_reply":"2023-10-09T13:59:57.484369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove numbers from each element while keeping spaces\ncleaned_concept_list = [' '.join(filter(str.isalpha, element.split())) for element in titles_with_descriptions]","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:33.113247Z","iopub.execute_input":"2023-10-10T10:01:33.113673Z","iopub.status.idle":"2023-10-10T10:01:33.119198Z","shell.execute_reply.started":"2023-10-10T10:01:33.113642Z","shell.execute_reply":"2023-10-10T10:01:33.118083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the fine-tuned model\n'''fine_tuned_model = GPT2LMHeadModel.from_pretrained('./fine-tuned-gpt2')  # Load from the fine-tuned model directory\n\n# Set the model to evaluation mode\nfine_tuned_model.eval()\ndefinitions={}\nfor i in range(len(cleaned_concept_list)):\n    \n# Define a starting prompt for text generation\n    prompt = \"define the \" + cleaned_concept_list[i] + \" concept.\"\n\n# Tokenize the prompt\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n# Generate text using the fine-tuned model\n    output = fine_tuned_model.generate(input_ids, max_length=200, num_return_sequences=1)\n\n# Decode and print the generated text\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    definitions[cleaned_concept_list[i]] = generated_text\n#print(definitions)'''","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:00:22.418948Z","iopub.execute_input":"2023-10-09T14:00:22.419277Z","iopub.status.idle":"2023-10-09T14:00:22.429615Z","shell.execute_reply.started":"2023-10-09T14:00:22.419252Z","shell.execute_reply":"2023-10-09T14:00:22.428154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create concepts dataframe\n* **We created our initial dataframe with 6 columns:**\n1. **Risk_concepts**: contains the name of the concept.\n2. **Relation_Type**: contains the type of relation with another concept or process (e.g. has input, is SubClass of ...).\n3. **Definition**: contains the isDefinedBy part, which refers to definition of the concept, plus the isDescribedBy part.\n4. **Clean_definition**: contains the isDefinedBy part, which refers to definition of the concept.\n5. **Figure**: contains the isDescribedin part, which refers to a figure.\n6. **Described_in**: contains the isDescribedin part, which refers to a section.","metadata":{}},{"cell_type":"code","source":"data_frame = pd.DataFrame()\ndata_frame['risk_concepts'] = [c for c in  titles_with_descriptions]\ndata_frame['Relation_Type'] = \"\"\ndata_frame['Definition'] = \"\"\ndata_frame['Clean_definition'] = \"\"\ndata_frame['Figure'] = \"\"\ndata_frame['Described_in'] = \"\"","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:37.682463Z","iopub.execute_input":"2023-10-10T10:01:37.683026Z","iopub.status.idle":"2023-10-10T10:01:37.713426Z","shell.execute_reply.started":"2023-10-10T10:01:37.682986Z","shell.execute_reply":"2023-10-10T10:01:37.712357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_concept_list = [' '.join(filter(str.isalpha, element.split())) for element in titles_with_descriptions]","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:39.833716Z","iopub.execute_input":"2023-10-10T10:01:39.834097Z","iopub.status.idle":"2023-10-10T10:01:39.839731Z","shell.execute_reply.started":"2023-10-10T10:01:39.834066Z","shell.execute_reply":"2023-10-10T10:01:39.838351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract and add Relation_Type","metadata":{}},{"cell_type":"code","source":"for idx, row in data_frame.iterrows():\n    if \":\" in str(row.risk_concepts) :\n            key=str(row.risk_concepts)[str(row.risk_concepts).index(\":\")+2:]\n            data_frame.at[idx, 'Relation_Type'] = f'Has {key}'","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:42.217536Z","iopub.execute_input":"2023-10-10T10:01:42.217870Z","iopub.status.idle":"2023-10-10T10:01:42.231759Z","shell.execute_reply.started":"2023-10-10T10:01:42.217843Z","shell.execute_reply":"2023-10-10T10:01:42.230417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Add the Definition","metadata":{}},{"cell_type":"code","source":"data_df=pd.read_csv(\"/kaggle/input/definition-vf/definition_vf.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:45.937867Z","iopub.execute_input":"2023-10-10T10:01:45.938219Z","iopub.status.idle":"2023-10-10T10:01:45.956094Z","shell.execute_reply.started":"2023-10-10T10:01:45.938190Z","shell.execute_reply":"2023-10-10T10:01:45.954937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx, row in data_frame.iterrows():\n    concept=' '.join(filter(str.isalpha, str(row.risk_concepts).split()))\n    def_string=\"\"\n    data_frame.at[idx,'risk_concepts'] = concept.lower()\n    for i in data_df[data_df[\"Concept\"]==concept.lower()][\"Definition\"].values:\n        def_string=str(i)\n    data_frame.at[idx,'Definition'] = def_string\n        \n#data_frame.at[len(concept_risk)-1, 'Definition']=text[text.find(data_frame.iloc[-1]['risk_concepts'])+ len(concept_risk[-1]):]","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:47.299996Z","iopub.execute_input":"2023-10-10T10:01:47.300518Z","iopub.status.idle":"2023-10-10T10:01:47.397836Z","shell.execute_reply.started":"2023-10-10T10:01:47.300477Z","shell.execute_reply":"2023-10-10T10:01:47.396846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for i in range(len(titles_with_descriptions) - 1):\n    #data_frame['Definition'][i] = definitions[titles_with_descriptions[i]]\n#data_frame.at[len(concept_risk)-1, 'Definition']=text[text.find(data_frame.iloc[-1]['risk_concepts'])+ len(concept_risk[-1]):]","metadata":{"execution":{"iopub.status.busy":"2023-10-07T15:41:20.876137Z","iopub.status.idle":"2023-10-07T15:41:20.877102Z","shell.execute_reply.started":"2023-10-07T15:41:20.876875Z","shell.execute_reply":"2023-10-07T15:41:20.876896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame['Clean_definition'] = data_frame['Definition']","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:50.660356Z","iopub.execute_input":"2023-10-10T10:01:50.660852Z","iopub.status.idle":"2023-10-10T10:01:50.668656Z","shell.execute_reply.started":"2023-10-10T10:01:50.660807Z","shell.execute_reply":"2023-10-10T10:01:50.667704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract the \"described in section\" and \"figure\" ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import GPT2Tokenizer, GPT2Model\nimport torch\nimport re\n\n# Supposons que vous ayez déjà chargé votre fichier CSV dans un dataframe df\n# df = pd.read_csv(\"def.csv\")\n\n# Chargez le tokenizer GPT-2 préentraîné\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Chargez le modèle GPT-2 préentraîné\nmodel = GPT2Model.from_pretrained(\"gpt2\")\n\n# Définissez une fonction pour extraire tous les numéros de section au format \"x.x.x.x\" après \"described in section\"\ndef extraire_numeros_sections(texte):\n    # Recherchez tous les textes \"described in section\" dans le texte\n    indices = [match.start() for match in re.finditer(\"described in section\", texte)]\n\n    numeros_sections = set()  # Utilisez un ensemble pour stocker les numéros de section uniques\n\n    for start_idx in indices:\n        # Extrait le texte après \"described in section\"\n        texte_apres_section = texte[start_idx + len(\"described in section\"):].strip()\n\n        # Ajoutez une séquence d'arrêt de texte au texte\n        texte_apres_section += tokenizer.eos_token\n\n        # Utilisez le tokenizer pour prétraiter le texte\n        texte_enc = tokenizer(texte_apres_section, return_tensors=\"pt\")\n\n        # Passez les données au modèle GPT-2 pour l'encodage\n        with torch.no_grad():\n            outputs = model(**texte_enc)\n\n        # Utilisez une expression régulière pour extraire les numéros de section (x.x.x.x)\n        numeros_section_match = re.findall(r'described in section (\\d+\\.\\d+\\.\\d+\\.\\d+)', texte)\n\n        if numeros_section_match:\n            numeros_sections.update(numeros_section_match)  # Utilisez \"update\" pour ajouter des éléments à l'ensemble\n\n    return [\"descriped in section \"+ str(i) for i in list(numeros_sections)]\n# Appliquez la fonction pour créer une nouvelle colonne contenant les numéros de section extraits\ndata_frame[\"Described_in\"] = data_frame[\"Definition\"].apply(extraire_numeros_sections)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:01:54.952019Z","iopub.execute_input":"2023-10-10T10:01:54.952390Z","iopub.status.idle":"2023-10-10T10:03:11.248124Z","shell.execute_reply.started":"2023-10-10T10:01:54.952362Z","shell.execute_reply":"2023-10-10T10:03:11.246939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_frame[\"Described_in\"][1]","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:03:11.250184Z","iopub.execute_input":"2023-10-10T10:03:11.250872Z","iopub.status.idle":"2023-10-10T10:03:11.258730Z","shell.execute_reply.started":"2023-10-10T10:03:11.250835Z","shell.execute_reply":"2023-10-10T10:03:11.257667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"item_list = []\nfor item in data_frame[\"Definition\"]:\n    item_list.append(item.split())\nitem_list_not_splitted = []\nfor item in data_frame[\"Definition\"]:\n    item_list_not_splitted.append(item)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:03:16.641016Z","iopub.execute_input":"2023-10-10T10:03:16.641390Z","iopub.status.idle":"2023-10-10T10:03:16.648031Z","shell.execute_reply.started":"2023-10-10T10:03:16.641349Z","shell.execute_reply":"2023-10-10T10:03:16.646956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract\ndescribed_list = []\nindex_list = []\nfor i in range(len(item_list)):\n    for j in range(len(item_list[i])):\n        if item_list[i][j] == \"described\" and item_list[i][j+1] == \"in\" and item_list[i][j+2] == \"section\" or item_list[i][j] == \"(described\" and item_list[i][j+1] == \"in\" and item_list[i][j+2] == \"section\":\n            described_list.append(item_list[i][j]+\" \"+item_list[i][j+1]+\" \"+item_list[i][j+2]+\" \"+item_list[i][j+3])\n            index_list.append(i)\n\n#load and remove\nfor i in range(len(index_list)):\n    data_frame['Clean_definition'].iloc[index_list[i]] = item_list_not_splitted[index_list[i]].replace(\"described in section\", \"\").replace(\"section\",\"\").replace(\"figure\",\"\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:02.426443Z","iopub.execute_input":"2023-10-10T10:04:02.426814Z","iopub.status.idle":"2023-10-10T10:04:02.522241Z","shell.execute_reply.started":"2023-10-10T10:04:02.426785Z","shell.execute_reply":"2023-10-10T10:04:02.521044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Extract\nfigure_list = []\nfigure_index_list = []\nfor i in range(len(item_list)):\n    for j in range(len(item_list[i])):\n        if item_list[i][j] == \"figure\":\n            figure_list.append(item_list[i][j]+\" \"+item_list[i][j+1])\n            #print(item_list[i][j+1])\n            figure_index_list.append(i)\n\n#load and remove\nfor i in range(len(figure_index_list)):\n    data_frame['Figure'].iloc[figure_index_list[i]] += \" \" + (figure_list[i])","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:04.478041Z","iopub.execute_input":"2023-10-10T10:04:04.478432Z","iopub.status.idle":"2023-10-10T10:04:04.495296Z","shell.execute_reply.started":"2023-10-10T10:04:04.478403Z","shell.execute_reply":"2023-10-10T10:04:04.494323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove \"described in section\" and \"figure\" from clean Definition","metadata":{}},{"cell_type":"code","source":"for i in range(len(figure_index_list)):\n    data_frame['Clean_definition'].iloc[figure_index_list[i]] = item_list_not_splitted[figure_index_list[i]].replace(\"described in section\", \"\")\n    data_frame['Clean_definition'].iloc[figure_index_list[i]] = item_list_not_splitted[figure_index_list[i]].replace(\"section\",\"\")\n    data_frame['Clean_definition'].iloc[figure_index_list[i]] = item_list_not_splitted[figure_index_list[i]].replace(\"figure\",\"\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:08.041904Z","iopub.execute_input":"2023-10-10T10:04:08.042259Z","iopub.status.idle":"2023-10-10T10:04:08.070870Z","shell.execute_reply.started":"2023-10-10T10:04:08.042231Z","shell.execute_reply":"2023-10-10T10:04:08.069725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing phase ","metadata":{}},{"cell_type":"code","source":"processing = pd.DataFrame()\nprocessing['risk_concepts'] = data_frame['risk_concepts']\nprocessing['Relation_Type'] = data_frame['Relation_Type']\nprocessing['Definition'] = data_frame['Definition']\nprocessing['Clean_definition'] = \"\"\nprocessing['Figure'] = data_frame['Figure']\nprocessing['Described_in'] = data_frame['Described_in']","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:34.333843Z","iopub.execute_input":"2023-10-10T10:04:34.334203Z","iopub.status.idle":"2023-10-10T10:04:34.344668Z","shell.execute_reply.started":"2023-10-10T10:04:34.334174Z","shell.execute_reply":"2023-10-10T10:04:34.343610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp =[]\nfor item in data_frame['Clean_definition']:\n    item = item\n    tmp.append(item)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:16.731169Z","iopub.execute_input":"2023-10-10T10:04:16.731832Z","iopub.status.idle":"2023-10-10T10:04:16.736344Z","shell.execute_reply.started":"2023-10-10T10:04:16.731805Z","shell.execute_reply":"2023-10-10T10:04:16.735385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Removing stopwords, digits and punctuation**","metadata":{}},{"cell_type":"code","source":"pattern = string.punctuation.replace('.','')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:25.802155Z","iopub.execute_input":"2023-10-10T10:04:25.803023Z","iopub.status.idle":"2023-10-10T10:04:25.808832Z","shell.execute_reply.started":"2023-10-10T10:04:25.802980Z","shell.execute_reply":"2023-10-10T10:04:25.807799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = ''\nfor i in range(len(tmp)):\n    ###### PUNCTUATION #########################\n    tmp[i] = tmp[i].replace('uu','')\n    tmp[i] = tmp[i].replace('\\n','')\n    tmp[i] = tmp[i].replace('—','')\n    tmp[i] = tmp[i].replace(',',' ')\n    tmp[i] = tmp[i].translate(str.maketrans('', '', pattern))\n    ###### Digits #############################\n    for ele in tmp[i]:\n        if ele.isdigit():\n            tmp[i] = tmp[i].replace(ele, ' ')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:38.168710Z","iopub.execute_input":"2023-10-10T10:04:38.169096Z","iopub.status.idle":"2023-10-10T10:04:38.188477Z","shell.execute_reply.started":"2023-10-10T10:04:38.169069Z","shell.execute_reply":"2023-10-10T10:04:38.187385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(tmp)):\n    processing['Clean_definition'].iloc[i] = tmp[i]","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:40.418707Z","iopub.execute_input":"2023-10-10T10:04:40.419105Z","iopub.status.idle":"2023-10-10T10:04:40.459539Z","shell.execute_reply.started":"2023-10-10T10:04:40.419075Z","shell.execute_reply":"2023-10-10T10:04:40.458365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Removing special characters**","metadata":{}},{"cell_type":"code","source":"def remove_special_characters(text):\n    \"\"\"\n        Remove special special characters, including symbols, emojis, and other graphic characters\n    \"\"\"\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\nprocessing['Clean_definition'] = processing['Clean_definition'].apply(lambda x: remove_special_characters(x))\nprocessing['Clean_definition'] = processing['Clean_definition'].apply(lambda s : s.lower())","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:43.038281Z","iopub.execute_input":"2023-10-10T10:04:43.038681Z","iopub.status.idle":"2023-10-10T10:04:43.058642Z","shell.execute_reply.started":"2023-10-10T10:04:43.038652Z","shell.execute_reply":"2023-10-10T10:04:43.057424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processing","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:45.072959Z","iopub.execute_input":"2023-10-10T10:04:45.073371Z","iopub.status.idle":"2023-10-10T10:04:45.098281Z","shell.execute_reply.started":"2023-10-10T10:04:45.073323Z","shell.execute_reply":"2023-10-10T10:04:45.097235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the REBEL model","metadata":{}},{"cell_type":"markdown","source":"REBEL is a text2text model trained by BabelScape by fine-tuning BART for translating a raw input sentence containing entities and implicit relations into a set of triplets that explicitly refer to those relations. It has been trained on more than 200 different relation types.","metadata":{}},{"cell_type":"code","source":"Triplets = pd.DataFrame()\nTriplets['risk_concepts'] = data_frame['risk_concepts']\nTriplets['head'] = \"\"\nTriplets['type'] = \"\"\nTriplets['tail'] = \"\"","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:53.233032Z","iopub.execute_input":"2023-10-10T10:04:53.233449Z","iopub.status.idle":"2023-10-10T10:04:53.241442Z","shell.execute_reply.started":"2023-10-10T10:04:53.233401Z","shell.execute_reply":"2023-10-10T10:04:53.240384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:55.041511Z","iopub.execute_input":"2023-10-10T10:04:55.041869Z","iopub.status.idle":"2023-10-10T10:04:55.046252Z","shell.execute_reply.started":"2023-10-10T10:04:55.041840Z","shell.execute_reply":"2023-10-10T10:04:55.045339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:04:57.193525Z","iopub.execute_input":"2023-10-10T10:04:57.194112Z","iopub.status.idle":"2023-10-10T10:05:17.826008Z","shell.execute_reply.started":"2023-10-10T10:04:57.194069Z","shell.execute_reply":"2023-10-10T10:05:17.824752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# From text to KB","metadata":{}},{"cell_type":"markdown","source":"The next step is to write a function that is able to parse the strings generated by REBEL and transform them into relation triplets. This function must take into account additional new tokens used while training the model.","metadata":{}},{"cell_type":"code","source":"def extract_relations_from_model_output(text):\n    relations = []\n    relation, subject, relation, object_ = '', '', '', ''\n    text = text.strip()\n    current = 'x'\n    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n    for token in text_replaced.split():\n        if token == \"<triplet>\":\n            current = 't'\n            if relation != '':\n                relations.append({\n                    'head': subject.strip(),\n                    'type': relation.strip(),\n                    'tail': object_.strip()\n                })\n                relation = ''\n            subject = ''\n        elif token == \"<subj>\":\n            current = 's'\n            if relation != '':\n                relations.append({\n                    'head': subject.strip(),\n                    'type': relation.strip(),\n                    'tail': object_.strip()\n                })\n            object_ = ''\n        elif token == \"<obj>\":\n            current = 'o'\n            relation = ''\n        else:\n            if current == 't':\n                subject += ' ' + token\n            elif current == 's':\n                object_ += ' ' + token\n            elif current == 'o':\n                relation += ' ' + token\n    if subject != '' and relation != '' and object_ != '':\n        relations.append({\n            'head': subject.strip(),\n            'type': relation.strip(),\n            'tail': object_.strip()\n        })\n    return relations","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:05:17.827989Z","iopub.execute_input":"2023-10-10T10:05:17.828351Z","iopub.status.idle":"2023-10-10T10:05:17.838526Z","shell.execute_reply.started":"2023-10-10T10:05:17.828317Z","shell.execute_reply":"2023-10-10T10:05:17.837426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class KB():\n    def __init__(self):\n        self.relations = []\n\n    def are_relations_equal(self, r1, r2):\n        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n\n    def exists_relation(self, r1):\n        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n\n    def add_relation(self, r):\n        if not self.exists_relation(r):\n            self.relations.append(r)\n\n    def save(self):\n        dict_list = []\n        for r in self.relations:\n            dict_list.append(r)\n        return(dict_list)\n        #df = pd.DataFrame(dict_list, columns=['head', 'type', 'tail'])\n        #print(dict_list)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:05:17.840091Z","iopub.execute_input":"2023-10-10T10:05:17.840712Z","iopub.status.idle":"2023-10-10T10:05:19.903048Z","shell.execute_reply.started":"2023-10-10T10:05:17.840676Z","shell.execute_reply":"2023-10-10T10:05:19.901935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def from_small_text_to_kb(text, verbose=False):\n    kb = KB()\n\n    # Tokenizer text\n    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n                            return_tensors='pt')\n    if verbose:\n        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n\n    # Generate\n    gen_kwargs = {\n        \"max_length\": 1000,\n        \"length_penalty\": 0,\n        \"num_beams\": 3,\n        \"num_return_sequences\": 3\n    }\n    \n    generated_tokens = model.generate(\n        **model_inputs,\n        **gen_kwargs,\n    )\n    \n    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n    \n    # create kb\n    for sentence_pred in decoded_preds:\n        relations = extract_relations_from_model_output(sentence_pred)\n        for r in relations:\n            kb.add_relation(r)\n\n    return kb","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:05:19.906138Z","iopub.execute_input":"2023-10-10T10:05:19.907266Z","iopub.status.idle":"2023-10-10T10:05:19.919930Z","shell.execute_reply.started":"2023-10-10T10:05:19.907228Z","shell.execute_reply":"2023-10-10T10:05:19.918807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:05:19.921216Z","iopub.execute_input":"2023-10-10T10:05:19.921622Z","iopub.status.idle":"2023-10-10T10:05:19.932370Z","shell.execute_reply.started":"2023-10-10T10:05:19.921589Z","shell.execute_reply":"2023-10-10T10:05:19.931282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relation_list =[]\nfor item in tqdm(processing['Clean_definition']):\n    kb = from_small_text_to_kb(item, verbose=False) #verbose = True to return the number of tokens in each sentence\n    relation_list.append(kb.save())","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:05:19.933767Z","iopub.execute_input":"2023-10-10T10:05:19.934702Z","iopub.status.idle":"2023-10-10T10:11:20.290471Z","shell.execute_reply.started":"2023-10-10T10:05:19.934667Z","shell.execute_reply":"2023-10-10T10:11:20.289366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_list =[]\nfor item in tqdm(relation_list):\n    for dic in item:\n        dict_list.append(dic)\ntriplet = pd.DataFrame(dict_list, columns=['head', 'type', 'tail'])","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.292584Z","iopub.execute_input":"2023-10-10T10:11:20.293407Z","iopub.status.idle":"2023-10-10T10:11:20.312284Z","shell.execute_reply.started":"2023-10-10T10:11:20.293345Z","shell.execute_reply":"2023-10-10T10:11:20.311366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"triplet","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.313621Z","iopub.execute_input":"2023-10-10T10:11:20.317907Z","iopub.status.idle":"2023-10-10T10:11:20.337420Z","shell.execute_reply.started":"2023-10-10T10:11:20.317848Z","shell.execute_reply":"2023-10-10T10:11:20.336427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_triplets = triplet.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.341957Z","iopub.execute_input":"2023-10-10T10:11:20.344871Z","iopub.status.idle":"2023-10-10T10:11:20.364272Z","shell.execute_reply.started":"2023-10-10T10:11:20.344793Z","shell.execute_reply":"2023-10-10T10:11:20.363115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_triplets","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.377273Z","iopub.execute_input":"2023-10-10T10:11:20.379875Z","iopub.status.idle":"2023-10-10T10:11:20.401239Z","shell.execute_reply.started":"2023-10-10T10:11:20.379835Z","shell.execute_reply":"2023-10-10T10:11:20.400251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tiplet_unique = triplet['head'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.406154Z","iopub.execute_input":"2023-10-10T10:11:20.408792Z","iopub.status.idle":"2023-10-10T10:11:20.416240Z","shell.execute_reply.started":"2023-10-10T10:11:20.408748Z","shell.execute_reply":"2023-10-10T10:11:20.415165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_triplet_unique = triplet.drop_duplicates(keep='first')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.422325Z","iopub.execute_input":"2023-10-10T10:11:20.425351Z","iopub.status.idle":"2023-10-10T10:11:20.435686Z","shell.execute_reply.started":"2023-10-10T10:11:20.425282Z","shell.execute_reply":"2023-10-10T10:11:20.434346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_triplet_unique = final_triplet_unique.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.441480Z","iopub.execute_input":"2023-10-10T10:11:20.443600Z","iopub.status.idle":"2023-10-10T10:11:20.451478Z","shell.execute_reply.started":"2023-10-10T10:11:20.443559Z","shell.execute_reply":"2023-10-10T10:11:20.450397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_triplet_unique","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.455231Z","iopub.execute_input":"2023-10-10T10:11:20.456655Z","iopub.status.idle":"2023-10-10T10:11:20.481950Z","shell.execute_reply.started":"2023-10-10T10:11:20.456617Z","shell.execute_reply":"2023-10-10T10:11:20.480907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textacy\nimport nltk\nfrom nltk.corpus import stopwords\nimport copy\n\n# Download the stopwords dataset (if not already downloaded)\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport spacy\nnlp_spacy = spacy.load(\"en_core_web_lg\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:20.486935Z","iopub.execute_input":"2023-10-10T10:11:20.489492Z","iopub.status.idle":"2023-10-10T10:11:29.352969Z","shell.execute_reply.started":"2023-10-10T10:11:20.489450Z","shell.execute_reply":"2023-10-10T10:11:29.351896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = ''\nfor item in processing['Clean_definition']:\n    text += ''.join(item)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:29.354507Z","iopub.execute_input":"2023-10-10T10:11:29.355275Z","iopub.status.idle":"2023-10-10T10:11:29.363419Z","shell.execute_reply.started":"2023-10-10T10:11:29.355248Z","shell.execute_reply":"2023-10-10T10:11:29.362289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_sent = text\nstop_words = set(stopwords.words('english'))\nword_tokens = word_tokenize(example_sent)\nfiltered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\nfiltered_sentence = []\n  \nfor w in word_tokens:\n    if w not in stop_words:\n        filtered_sentence.append(w)\ntest_text = ' '.join(filtered_sentence)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:29.365178Z","iopub.execute_input":"2023-10-10T10:11:29.366283Z","iopub.status.idle":"2023-10-10T10:11:29.567270Z","shell.execute_reply.started":"2023-10-10T10:11:29.366248Z","shell.execute_reply":"2023-10-10T10:11:29.566253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract subject verb and object using Textacy","metadata":{}},{"cell_type":"code","source":"import textacy\ntriple_list = []\nfor sentence in test_text.split('.'):\n    t1 = nlp_spacy(sentence)\n    triple = textacy.extract.subject_verb_object_triples(t1)\n    if triple:\n        triple_to_list = list(triple)\n        triple_list.append(pd.DataFrame(triple_to_list))\nsvo=pd.concat(triple_list, axis=0) # this should concat all dfs on top of one another using axis=0\nsvo.columns=['subject','verb','object'] # change your columns on teh final df","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:29.568571Z","iopub.execute_input":"2023-10-10T10:11:29.568938Z","iopub.status.idle":"2023-10-10T10:11:48.282327Z","shell.execute_reply.started":"2023-10-10T10:11:29.568906Z","shell.execute_reply":"2023-10-10T10:11:48.281347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svo","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:53.312222Z","iopub.execute_input":"2023-10-10T10:11:53.312722Z","iopub.status.idle":"2023-10-10T10:11:53.345597Z","shell.execute_reply.started":"2023-10-10T10:11:53.312686Z","shell.execute_reply":"2023-10-10T10:11:53.344505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svo = svo.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:57.691192Z","iopub.execute_input":"2023-10-10T10:11:57.691606Z","iopub.status.idle":"2023-10-10T10:11:57.696865Z","shell.execute_reply.started":"2023-10-10T10:11:57.691573Z","shell.execute_reply":"2023-10-10T10:11:57.695838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_svo = copy.deepcopy(svo) ","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:03.164537Z","iopub.execute_input":"2023-10-10T10:12:03.164893Z","iopub.status.idle":"2023-10-10T10:12:03.170435Z","shell.execute_reply.started":"2023-10-10T10:12:03.164860Z","shell.execute_reply":"2023-10-10T10:12:03.169262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx,row in final_svo.iterrows():\n    for c in final_svo.columns:\n        for i in row[c]:\n            if isinstance(i, spacy.tokens.span.Span):\n                row[c] = i.text\n            else:\n                row[c] = i","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:04.808085Z","iopub.execute_input":"2023-10-10T10:12:04.808460Z","iopub.status.idle":"2023-10-10T10:12:04.894079Z","shell.execute_reply.started":"2023-10-10T10:12:04.808431Z","shell.execute_reply":"2023-10-10T10:12:04.893089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for idx,item in enumerate(final_svo['subject']):\n    if isinstance(item, spacy.tokens.token.Token):\n        item = item.text\n        final_svo.iloc[idx]['subject'] = item\n    else:\n        item = item\n        final_svo.iloc[idx]['subject'] = item\nfor idx,item in enumerate(final_svo['verb']):\n    if isinstance(item, spacy.tokens.token.Token):\n        item = item.text\n        final_svo.iloc[idx]['verb'] = item\n    else:\n        item = item\n        final_svo.iloc[idx]['verb'] = item\nfor idx,item in enumerate(final_svo['object']):\n    if isinstance(item, spacy.tokens.token.Token):\n        item = item.text\n        final_svo.iloc[idx]['object'] = item\n    else:\n        item = item\n        final_svo.iloc[idx]['object'] = item","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:07.659931Z","iopub.execute_input":"2023-10-10T10:12:07.660344Z","iopub.status.idle":"2023-10-10T10:12:07.754621Z","shell.execute_reply.started":"2023-10-10T10:12:07.660288Z","shell.execute_reply":"2023-10-10T10:12:07.753563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concept_list = []\nconcept_index = []\nfor _,row in final_svo.iterrows():\n    item = row['subject']\n    for i in tiplet_unique:\n        if (i in item):\n            concept_list.append(item)\n            concept_index.append(list(final_svo['subject']).index(item))\nsvo_unique = dict(zip(concept_index, concept_list))","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:48.678116Z","iopub.execute_input":"2023-10-10T10:11:48.680647Z","iopub.status.idle":"2023-10-10T10:11:48.892703Z","shell.execute_reply.started":"2023-10-10T10:11:48.680606Z","shell.execute_reply":"2023-10-10T10:11:48.891749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_svo_unique = final_svo.drop_duplicates(keep='first')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:33.904224Z","iopub.execute_input":"2023-10-10T10:12:33.904746Z","iopub.status.idle":"2023-10-10T10:12:33.912084Z","shell.execute_reply.started":"2023-10-10T10:12:33.904713Z","shell.execute_reply":"2023-10-10T10:12:33.911050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_svo_unique","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:35.444389Z","iopub.execute_input":"2023-10-10T10:12:35.444748Z","iopub.status.idle":"2023-10-10T10:12:35.457158Z","shell.execute_reply.started":"2023-10-10T10:12:35.444722Z","shell.execute_reply":"2023-10-10T10:12:35.455778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_svo = final_svo.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:39.043168Z","iopub.execute_input":"2023-10-10T10:12:39.044203Z","iopub.status.idle":"2023-10-10T10:12:39.049820Z","shell.execute_reply.started":"2023-10-10T10:12:39.044156Z","shell.execute_reply":"2023-10-10T10:12:39.048672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_svo['subject'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:40.731733Z","iopub.execute_input":"2023-10-10T10:12:40.732100Z","iopub.status.idle":"2023-10-10T10:12:40.740673Z","shell.execute_reply.started":"2023-10-10T10:12:40.732071Z","shell.execute_reply":"2023-10-10T10:12:40.739514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final data frame","metadata":{}},{"cell_type":"code","source":"for idx,item in enumerate(processing['Described_in']):\n    processing['Described_in'].iloc[idx] = \",\".join(item)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:43.936689Z","iopub.execute_input":"2023-10-10T10:12:43.937031Z","iopub.status.idle":"2023-10-10T10:12:43.979551Z","shell.execute_reply.started":"2023-10-10T10:12:43.937004Z","shell.execute_reply":"2023-10-10T10:12:43.978468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processing['Described_in']","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:44.834101Z","iopub.execute_input":"2023-10-10T10:12:44.834794Z","iopub.status.idle":"2023-10-10T10:12:44.842695Z","shell.execute_reply.started":"2023-10-10T10:12:44.834760Z","shell.execute_reply":"2023-10-10T10:12:44.841538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final = pd.DataFrame()\ndf_final['Concepts'] = data_frame['risk_concepts']\ndf_final['Type_relation'] = ''\ndf_final['Concept_of_type_relation'] = ''\ndf_final['Definition'] = processing['Clean_definition']\ndf_final['Synonym'] = ''\ndf_final['Reference'] = processing['Figure'] + processing['Described_in']\ndf_final['Process_name'] = ''","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:47.707324Z","iopub.execute_input":"2023-10-10T10:12:47.707669Z","iopub.status.idle":"2023-10-10T10:12:47.717764Z","shell.execute_reply.started":"2023-10-10T10:12:47.707643Z","shell.execute_reply":"2023-10-10T10:12:47.716800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding final_triplet_unique relation and its range\n","metadata":{}},{"cell_type":"code","source":"for idx1,row1 in final_triplet_unique.iterrows():\n    for idx2,row2 in df_final.iterrows():\n        if row1['head'] in row2['Concepts']:\n            df_final.at[idx2,'Type_relation']=row1['type']\n            #row2['Type_relation'] = row1['type']\n            df_final.at[idx2,'Concept_of_type_relation']=row1['tail']","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:51.683027Z","iopub.execute_input":"2023-10-10T10:12:51.683421Z","iopub.status.idle":"2023-10-10T10:12:52.612188Z","shell.execute_reply.started":"2023-10-10T10:12:51.683390Z","shell.execute_reply":"2023-10-10T10:12:52.611122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Adding svo_unique  and its range\n","metadata":{}},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\")\nfor key, value in svo_unique.items():\n    for key2,row in df_final.iterrows():\n        if value in row['Concepts']:  \n            df_final.at[key2,'Process_name'] = stemmer.stem(final_svo.iloc[key]['verb'])+\" \"+(final_svo.iloc[key]['object'])","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:12:59.096285Z","iopub.execute_input":"2023-10-10T10:12:59.096709Z","iopub.status.idle":"2023-10-10T10:12:59.783666Z","shell.execute_reply.started":"2023-10-10T10:12:59.096673Z","shell.execute_reply":"2023-10-10T10:12:59.782620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Similarity case","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load pre-trained BERT model and tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:49.140845Z","iopub.status.idle":"2023-10-10T10:11:49.141585Z","shell.execute_reply.started":"2023-10-10T10:11:49.141342Z","shell.execute_reply":"2023-10-10T10:11:49.141366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:49.142912Z","iopub.status.idle":"2023-10-10T10:11:49.143618Z","shell.execute_reply.started":"2023-10-10T10:11:49.143367Z","shell.execute_reply":"2023-10-10T10:11:49.143391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef similarity(sentence1,sentence2):\n    # Tokenize and encode the sentences\n    inputs1 = tokenizer(sentence1, return_tensors=\"pt\", padding=True, truncation=True)\n    inputs2 = tokenizer(sentence2, return_tensors=\"pt\", padding=True, truncation=True)\n\n    # Get the embeddings\n    with torch.no_grad():\n        outputs1 = model(**inputs1)\n        outputs2 = model(**inputs2)\n\n    # Take the embeddings of the [CLS] token\n    embeddings1 = outputs1.last_hidden_state[:, 0, :]\n    embeddings2 = outputs2.last_hidden_state[:, 0, :]\n\n    # Calculate cosine similarity\n    similarity_score = cosine_similarity(embeddings1, embeddings2)[0][0]\n\n    # Compare the similarity score\n    return similarity_score ","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:11:49.144898Z","iopub.status.idle":"2023-10-10T10:11:49.145596Z","shell.execute_reply.started":"2023-10-10T10:11:49.145362Z","shell.execute_reply":"2023-10-10T10:11:49.145385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence1 = \"i read 7 books\"\nsentence2 = \"i have 5 books\"\nsimilarity(sentence1,sentence2)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T14:29:08.698772Z","iopub.execute_input":"2023-10-09T14:29:08.699315Z","iopub.status.idle":"2023-10-09T14:29:08.880293Z","shell.execute_reply.started":"2023-10-09T14:29:08.699279Z","shell.execute_reply":"2023-10-09T14:29:08.879285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final = df_final.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:13:08.152855Z","iopub.execute_input":"2023-10-10T10:13:08.153761Z","iopub.status.idle":"2023-10-10T10:13:08.161487Z","shell.execute_reply.started":"2023-10-10T10:13:08.153720Z","shell.execute_reply":"2023-10-10T10:13:08.160350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0, (df_final.shape[0])-1):\n    text1= df_final.iloc[i]['Definition']\n    print(i)\n    for j in range(i+1,df_final.shape[0]):\n        text2=df_final.iloc[j]['Definition']\n        \n        if similarity(text1,text2) == 1 :\n            df_final.iloc[i]['Synonym'] += ',' + df_final.iloc[j]['Concepts']\n            for ele in df_final['Synonym'].iloc[i]:\n                if ele.isdigit():\n                    df_final['Synonym'].iloc[i] = df_final['Synonym'].iloc[i].replace(ele, ' ')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:13:27.654619Z","iopub.execute_input":"2023-10-10T10:13:27.655015Z","iopub.status.idle":"2023-10-10T10:13:27.674956Z","shell.execute_reply.started":"2023-10-10T10:13:27.654970Z","shell.execute_reply":"2023-10-10T10:13:27.673585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation \n","metadata":{}},{"cell_type":"code","source":"all_data_combined = df_final['Concepts'].tolist() + df_final['Type_relation'].tolist()+ df_final['Definition'].tolist()+df_final['Synonym'].tolist()+df_final['Reference'].tolist()+df_final['Process_name'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:19:22.422800Z","iopub.execute_input":"2023-10-10T10:19:22.423201Z","iopub.status.idle":"2023-10-10T10:19:22.428845Z","shell.execute_reply.started":"2023-10-10T10:19:22.423174Z","shell.execute_reply":"2023-10-10T10:19:22.427803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ch=''\nfor e in all_data_combined:\n    ch += '. '+str(e)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:19:24.518283Z","iopub.execute_input":"2023-10-10T10:19:24.519283Z","iopub.status.idle":"2023-10-10T10:19:24.525484Z","shell.execute_reply.started":"2023-10-10T10:19:24.519236Z","shell.execute_reply":"2023-10-10T10:19:24.524101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"proc = project_risk_management","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:19:33.291118Z","iopub.execute_input":"2023-10-10T10:19:33.291538Z","iopub.status.idle":"2023-10-10T10:19:33.296622Z","shell.execute_reply.started":"2023-10-10T10:19:33.291509Z","shell.execute_reply":"2023-10-10T10:19:33.295485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rouge = Rouge()\nscore=rouge.get_scores(ch, proc, avg=True)\nprint('Precision = '+str(score['rouge-1']['p']))\nprint('Recall = ' +str(score['rouge-1']['r']))\nprint('f-measure = '+str(score['rouge-1']['f']))","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:21:06.303976Z","iopub.execute_input":"2023-10-10T10:21:06.304371Z","iopub.status.idle":"2023-10-10T10:24:13.848369Z","shell.execute_reply.started":"2023-10-10T10:21:06.304337Z","shell.execute_reply":"2023-10-10T10:24:13.847373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Semantic Search\n","metadata":{}},{"cell_type":"markdown","source":"Semantic search seeks to improve search accuracy by understanding the content of the search query. In contrast to traditional search engines, which only find documents based on lexical matches, semantic search can also find synonyms.\n\nIn fact, this type of search makes browsing more complete by understanding almost exactly what the user is trying to ask, instead of simply matching keywords to pages. The idea behind semantic search is to embed all entries in your corpus, which can be sentences, paragraphs, or documents, into a vector space. At search time, the query is embedded into the same vector space and the closest embedding from your corpus is found. These entries should have a high semantic overlap with the query","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimport numpy as np\nimport faiss\nimport time\nfrom sentence_transformers import CrossEncoder\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:43:26.635952Z","iopub.execute_input":"2023-10-10T10:43:26.636470Z","iopub.status.idle":"2023-10-10T10:43:26.643401Z","shell.execute_reply.started":"2023-10-10T10:43:26.636430Z","shell.execute_reply":"2023-10-10T10:43:26.642376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer('msmarco-distilbert-base-dot-prod-v3')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:33:47.406165Z","iopub.execute_input":"2023-10-10T10:33:47.406863Z","iopub.status.idle":"2023-10-10T10:33:48.399254Z","shell.execute_reply.started":"2023-10-10T10:33:47.406830Z","shell.execute_reply":"2023-10-10T10:33:48.398163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Definition_list = df_final.Definition.tolist()","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:33:51.144811Z","iopub.execute_input":"2023-10-10T10:33:51.145234Z","iopub.status.idle":"2023-10-10T10:33:51.150385Z","shell.execute_reply.started":"2023-10-10T10:33:51.145188Z","shell.execute_reply":"2023-10-10T10:33:51.149320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**FAISS: (Facebook AI Similarity Search)** is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves the limitations of traditional query search engines that are optimized for hash-based searches and provides more scalable similarity search functions.","metadata":{}},{"cell_type":"code","source":"encoded_data = model.encode(Definition_list)\nencoded_data = np.asarray(encoded_data.astype('float32'))\nindex = faiss.IndexIDMap(faiss.IndexFlatIP(768))\nindex.add_with_ids(encoded_data, np.array(range(0, len(df_final))))\nfaiss.write_index(index, 'Definition.index')\n","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:37:18.177658Z","iopub.execute_input":"2023-10-10T10:37:18.178034Z","iopub.status.idle":"2023-10-10T10:37:18.416432Z","shell.execute_reply.started":"2023-10-10T10:37:18.178004Z","shell.execute_reply":"2023-10-10T10:37:18.415316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_related_definition(dataframe_idx):\n    info = df_final.iloc[dataframe_idx]\n    meta_dict = dict()\n    meta_dict['Concepts'] = info['Concepts']\n    meta_dict['Definition'] = info['Definition']\n    return meta_dict\n\ndef search(query, top_k, index, model):\n    t = time.time()\n    query_vector = model.encode([query])\n    top_k = index.search(query_vector, top_k)\n    print('>>>> Results in Total Time: {}'.format(time.time()-t))\n    top_k_ids = top_k[1].tolist()[0]\n    top_k_ids = list(np.unique(top_k_ids))\n    results =  [fetch_related_definition(idx) for idx in top_k_ids]\n    return results","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:38:04.123036Z","iopub.execute_input":"2023-10-10T10:38:04.123451Z","iopub.status.idle":"2023-10-10T10:38:04.130703Z","shell.execute_reply.started":"2023-10-10T10:38:04.123420Z","shell.execute_reply":"2023-10-10T10:38:04.129531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query=\"plan risk management\"","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:38:52.435408Z","iopub.execute_input":"2023-10-10T10:38:52.435772Z","iopub.status.idle":"2023-10-10T10:38:52.440275Z","shell.execute_reply.started":"2023-10-10T10:38:52.435746Z","shell.execute_reply":"2023-10-10T10:38:52.439332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results=search(query, top_k=5, index=index, model=model)\nprint(\"\\n\")\nfor result in results:\n    print('\\t',result)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:39:15.193781Z","iopub.execute_input":"2023-10-10T10:39:15.194147Z","iopub.status.idle":"2023-10-10T10:39:15.240976Z","shell.execute_reply.started":"2023-10-10T10:39:15.194117Z","shell.execute_reply":"2023-10-10T10:39:15.240007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Load our cross-encoder. Use fast tokenizer to speed up the tokenization\ncross_model = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-6', max_length=512)","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:40:51.678389Z","iopub.execute_input":"2023-10-10T10:40:51.678758Z","iopub.status.idle":"2023-10-10T10:40:57.940507Z","shell.execute_reply.started":"2023-10-10T10:40:51.678729Z","shell.execute_reply":"2023-10-10T10:40:57.939582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_score(model_inputs):\n    scores = cross_model.predict(model_inputs)\n    return scores\n\nmodel_inputs = [[query,item['Definition']] for item in results]\nscores = cross_score(model_inputs)\n# sort the scores in decreasing order\nranked_results = [{'Definition': inp['Definition'], 'Score': score} for inp, score in zip(results, scores)]\nprint(\"\\n\")\nfor result in ranked_results:\n    print('\\t',pprint(result))","metadata":{"execution":{"iopub.status.busy":"2023-10-10T10:43:31.030202Z","iopub.execute_input":"2023-10-10T10:43:31.030619Z","iopub.status.idle":"2023-10-10T10:43:31.081801Z","shell.execute_reply.started":"2023-10-10T10:43:31.030591Z","shell.execute_reply":"2023-10-10T10:43:31.080691Z"},"trusted":true},"execution_count":null,"outputs":[]}]}